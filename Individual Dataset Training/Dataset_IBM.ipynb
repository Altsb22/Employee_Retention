{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e64e8c",
   "metadata": {},
   "source": [
    "This is going to be the Jupyter Notebook for the IBM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1269c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1973ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ibm_data = pd.read_csv(\"C:/Users/alt98/Desktop/Machine Learning Project/Employee_Retention/Dataset - IBM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_data = load_ibm_data.copy()\n",
    "#make sure it works\n",
    "ibm_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles to lowercase\n",
    "ibm_data.columns = ibm_data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5556d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what kind of data we have here\n",
    "ibm_data.info()\n",
    "# o null values in the dataset, so we can proceed with the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts for all category-type columns\n",
    "categorical_columns = ibm_data.select_dtypes(include='object')  # Select category-type columns\n",
    "value_counts = {col: categorical_columns[col].value_counts() for col in categorical_columns.columns}\n",
    "\n",
    "# Print the value counts for each column\n",
    "for col, counts in value_counts.items():\n",
    "    print(f\"Value counts for column '{col}':\")\n",
    "    print(counts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look numerical columns\n",
    "ibm_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a57894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the numeric columns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "ibm_data.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns that are not needed for the analysis\n",
    "ibm_data.drop(columns=['employeenumber', 'standardhours'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a training set\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bca8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a hash of each instance identifier \n",
    "from zlib import crc32\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an identifier column using the row index\n",
    "ibm_data_with_id = ibm_data.reset_index()\n",
    "train_set, test_set =  split_train_test_by_id(ibm_data_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7488f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are looking to measure attrition so let's visualize the ratio of yes to no.\n",
    "ibm_data['attrition'].hist()\n",
    "plt.show()\n",
    "#the ratio of yes to know is adequate for the exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39724fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up stratified sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(ibm_data, ibm_data['attrition']):\n",
    "    strat_train_set = ibm_data.loc[train_index]\n",
    "    strat_test_set = ibm_data.loc[test_index]\n",
    "#check the stratified sampling\n",
    "strat_test_set['attrition'].value_counts() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change categorical values to dummy variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "categorical_columns_1hot = cat_encoder.fit_transform(categorical_columns)\n",
    "categorical_columns_1hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14906e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change encoded columns to float64\n",
    "categorical_columns_1hot = categorical_columns_1hot.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62521e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                         NaN\n",
       "attrition                   NaN\n",
       "dailyrate                   NaN\n",
       "distancefromhome            NaN\n",
       "hourlyrate                  NaN\n",
       "                             ..\n",
       "stockoptionlevel_nan        NaN\n",
       "worklifebalance_Bad         NaN\n",
       "worklifebalance_Good        NaN\n",
       "worklifebalance_Very Bad    NaN\n",
       "worklifebalance_Very Good   NaN\n",
       "Name: attrition, Length: 81, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the 'attrition' column to numeric values in the original dataframe\n",
    "ibm_data['attrition'] = ibm_data['attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Create a correlation matrix using only numeric columns\n",
    "numeric_ibm_data = ibm_data.select_dtypes(include=['number'])\n",
    "corr_matrix = numeric_ibm_data.corr()\n",
    "\n",
    "# Sort by correlation with 'attrition'\n",
    "# Convert the one-hot encoded sparse matrix to a dense DataFrame\n",
    "categorical_columns_1hot_df = pd.DataFrame(categorical_columns_1hot.toarray(), \n",
    "                                           columns=cat_encoder.get_feature_names_out(categorical_columns.columns),\n",
    "                                           index=ibm_data.index)\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original dataframe\n",
    "ibm_data_encoded = pd.concat([numeric_ibm_data, categorical_columns_1hot_df], axis=1)\n",
    "\n",
    "# Create a correlation matrix using the updated dataframe\n",
    "corr_matrix = ibm_data_encoded.corr()\n",
    "\n",
    "# Sort by correlation with 'attrition'\n",
    "corr_matrix['attrition'].sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
